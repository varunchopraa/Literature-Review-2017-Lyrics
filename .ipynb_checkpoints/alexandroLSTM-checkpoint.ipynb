{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varunchopra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/varunchopra/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/varunchopra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varunchopra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = (stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating lyricsProcessed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "genresList = ['blues_lyrics', 'country_lyrics', 'disco_lyrics', 'hiphop_lyrics', 'metal_lyrics', 'pop_lyrics', 'reggae_lyrics', 'rock_lyrics']\n",
    "genreLyrics = dict()\n",
    "lyricStopped = {\n",
    "    'blues': list(),\n",
    "    'country': list(),\n",
    "    'disco': list(),\n",
    "    'hiphop': list(),\n",
    "    'metal': list(),\n",
    "    'pop': list(),\n",
    "    'reggae': list(),\n",
    "    'rock': list(),\n",
    "}\n",
    "\n",
    "header = ['filename', 'lyrics', 'genre']\n",
    "\n",
    "with open(f'lyricsProcessed.csv', 'w', newline = \"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "for genreName in genresList:\n",
    "    genre = sorted(os.listdir(f'../genres2/{genreName}'))\n",
    "    \n",
    "    for song in genre:\n",
    "#         print(genreName, song)\n",
    "\n",
    "        lyricStopped = []\n",
    "        \n",
    "        if song == '.DS_Store':\n",
    "            continue\n",
    "            \n",
    "        path = f'../genres2/{genreName}/{song}'\n",
    "        songName = open(path, 'r')\n",
    "        lyric = songName.read().lower()\n",
    "        lyric = lyric.translate(str.maketrans('','',string.punctuation))\n",
    "        \n",
    "        words = lyric.split()\n",
    "\n",
    "        # removing numbers\n",
    "        words = [x for x in words if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "\n",
    "        # lemmatized words\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "        words = [lmtzr.lemmatize(x) for x in words]\n",
    "\n",
    "        # removing the stopwords\n",
    "        for l in words:\n",
    "            if l not in stop_words:\n",
    "                lyricStopped.append(l)\n",
    "\n",
    "        # stemming\n",
    "        porter = PorterStemmer()\n",
    "        lyricStopped = [porter.stem(word) for word in lyricStopped]\n",
    "        lyric = ''\n",
    "        for word in lyricStopped:\n",
    "            lyric += word + ' '\n",
    "        \n",
    "        with open(f'lyricsProcessed.csv', 'a', newline = \"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([song, lyric, genreName.replace('_lyrics','')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(646,)\n",
      "(646,)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('lyricsProcessed.csv')\n",
    "X = dataset.iloc[:, 1].values\n",
    "y = dataset.iloc[:, 2].values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(646,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics = X[:]\n",
    "lyrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(646, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "lyric_encoded=le.fit_transform(lyrics.astype(str))\n",
    "lyric_encoded = np.reshape(lyric_encoded,(-1,1))\n",
    "lyric_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(646, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(646,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.row_stack(lyric_encoded)\n",
    "print(features.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validating LSTM on lyrics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 581 samples, validate on 65 samples\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 1s 982us/step - loss: 2.2378 - accuracy: 0.1102 - val_loss: 2.0834 - val_accuracy: 0.1077\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 0s 174us/step - loss: 2.1463 - accuracy: 0.1205 - val_loss: 2.0605 - val_accuracy: 0.2000\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 0s 146us/step - loss: 2.1222 - accuracy: 0.1343 - val_loss: 2.0541 - val_accuracy: 0.2000\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 0s 148us/step - loss: 2.1225 - accuracy: 0.1497 - val_loss: 2.0523 - val_accuracy: 0.2000\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 0s 140us/step - loss: 2.1261 - accuracy: 0.1411 - val_loss: 2.0512 - val_accuracy: 0.2154\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 0s 140us/step - loss: 2.0862 - accuracy: 0.1497 - val_loss: 2.0538 - val_accuracy: 0.1846\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 0s 140us/step - loss: 2.0688 - accuracy: 0.1480 - val_loss: 2.0526 - val_accuracy: 0.1846\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 0s 140us/step - loss: 2.0158 - accuracy: 0.1721 - val_loss: 2.0501 - val_accuracy: 0.2000\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 0s 142us/step - loss: 1.9882 - accuracy: 0.2203 - val_loss: 2.0475 - val_accuracy: 0.2615\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 0s 142us/step - loss: 1.9674 - accuracy: 0.2392 - val_loss: 2.0428 - val_accuracy: 0.2769\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 0s 142us/step - loss: 1.9346 - accuracy: 0.2513 - val_loss: 2.0377 - val_accuracy: 0.2769\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 0s 142us/step - loss: 1.8658 - accuracy: 0.3236 - val_loss: 2.0310 - val_accuracy: 0.2308\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 0s 145us/step - loss: 1.8045 - accuracy: 0.3701 - val_loss: 2.0176 - val_accuracy: 0.2154\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 0s 143us/step - loss: 1.7787 - accuracy: 0.3941 - val_loss: 2.0057 - val_accuracy: 0.2769\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 0s 145us/step - loss: 1.6538 - accuracy: 0.5077 - val_loss: 1.9989 - val_accuracy: 0.2462\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 0s 155us/step - loss: 1.6191 - accuracy: 0.5129 - val_loss: 1.9900 - val_accuracy: 0.2462\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 0s 155us/step - loss: 1.5081 - accuracy: 0.5886 - val_loss: 1.9785 - val_accuracy: 0.2769\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 0s 149us/step - loss: 1.3916 - accuracy: 0.6368 - val_loss: 1.9619 - val_accuracy: 0.2154\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 0s 145us/step - loss: 1.2963 - accuracy: 0.6919 - val_loss: 1.9481 - val_accuracy: 0.2308\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 0s 146us/step - loss: 1.1990 - accuracy: 0.7298 - val_loss: 1.9316 - val_accuracy: 0.2923\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense, GlobalMaxPooling1D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = pd.read_csv('lyrics.csv')\n",
    "X = dataset.iloc[:, 1].values\n",
    "y = dataset.iloc[:, 2].values\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_enc = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y_enc, test_size = 0.1, random_state = 15)\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(3000, 100, input_length=1, trainable=True))\n",
    "model.add(LSTM(50, activation='sigmoid', return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 56us/step\n",
      "Test set\n",
      "  Loss: 1.932\n",
      "  Accuracy: 0.292\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validating LSTM on lyricsProcessed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 581 samples, validate on 65 samples\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 1s 965us/step - loss: 2.1357 - accuracy: 0.1515 - val_loss: 2.0797 - val_accuracy: 0.1846\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 0s 184us/step - loss: 2.1219 - accuracy: 0.1515 - val_loss: 2.0674 - val_accuracy: 0.1846\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 0s 144us/step - loss: 2.1053 - accuracy: 0.1446 - val_loss: 2.0622 - val_accuracy: 0.1846\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 0s 138us/step - loss: 2.1130 - accuracy: 0.1343 - val_loss: 2.0632 - val_accuracy: 0.1846\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 0s 140us/step - loss: 2.0563 - accuracy: 0.1773 - val_loss: 2.0612 - val_accuracy: 0.1846\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 0s 141us/step - loss: 2.0685 - accuracy: 0.1601 - val_loss: 2.0601 - val_accuracy: 0.1846\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 0s 140us/step - loss: 2.0467 - accuracy: 0.1790 - val_loss: 2.0574 - val_accuracy: 0.2615\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 0s 142us/step - loss: 2.0288 - accuracy: 0.1670 - val_loss: 2.0540 - val_accuracy: 0.2923\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 0s 144us/step - loss: 1.9971 - accuracy: 0.2117 - val_loss: 2.0496 - val_accuracy: 0.2769\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 0s 143us/step - loss: 1.9554 - accuracy: 0.2530 - val_loss: 2.0460 - val_accuracy: 0.2462\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 0s 145us/step - loss: 1.9046 - accuracy: 0.2909 - val_loss: 2.0375 - val_accuracy: 0.2308\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 0s 145us/step - loss: 1.8408 - accuracy: 0.3804 - val_loss: 2.0301 - val_accuracy: 0.2923\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 0s 144us/step - loss: 1.7963 - accuracy: 0.3959 - val_loss: 2.0245 - val_accuracy: 0.2769\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 0s 144us/step - loss: 1.7420 - accuracy: 0.4251 - val_loss: 2.0142 - val_accuracy: 0.2769\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 0s 143us/step - loss: 1.6512 - accuracy: 0.5077 - val_loss: 2.0024 - val_accuracy: 0.2615\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 0s 144us/step - loss: 1.5425 - accuracy: 0.5680 - val_loss: 1.9928 - val_accuracy: 0.2769\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 0s 146us/step - loss: 1.4793 - accuracy: 0.5869 - val_loss: 1.9786 - val_accuracy: 0.2308\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 0s 145us/step - loss: 1.3688 - accuracy: 0.6695 - val_loss: 1.9685 - val_accuracy: 0.2462\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 0s 143us/step - loss: 1.2720 - accuracy: 0.6885 - val_loss: 1.9542 - val_accuracy: 0.2462\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 0s 144us/step - loss: 1.1827 - accuracy: 0.7263 - val_loss: 1.9398 - val_accuracy: 0.2308\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense, GlobalMaxPooling1D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = pd.read_csv('lyricsProcessed.csv')\n",
    "X = dataset.iloc[:, 1].values\n",
    "y = dataset.iloc[:, 2].values\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_enc = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y_enc, test_size = 0.1, random_state = 15)\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "modelP = Sequential()\n",
    "modelP.add(Embedding(3000, 100, input_length=1, trainable=True))\n",
    "modelP.add(LSTM(50, activation='sigmoid', return_sequences=True))\n",
    "modelP.add(Dropout(0.5))\n",
    "modelP.add(GlobalMaxPooling1D())\n",
    "modelP.add(Dense(8, activation='softmax'))\n",
    "modelP.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "history = modelP.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 53us/step\n",
      "Test set\n",
      "  Loss: 1.940\n",
      "  Accuracy: 0.231\n"
     ]
    }
   ],
   "source": [
    "accr = modelP.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using LSTM trained on unprocessed lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i met a gin-soaked, bar-room queen in memphis\n",
      "she tried to take me upstairs for a ride\n",
      "she had to heave me right across shoulder\n",
      "'cause i just can't seem to drink you off my mind\n",
      "it's the honky tonk women\n",
      "gimme, gimme, gimme the honky tonk blues\n",
      "i laid a divorcée in new york city\n",
      "i had to put up some kind of a fight\n",
      "the lady then she covered me with roses\n",
      "she blew my nose and then she blew my mind\n",
      "it's the honky tonk women\n",
      "gimme, gimme, gimme the honky tonk blues\n",
      "it's the honky tonk women\n",
      "gimme, gimme, gimme the honky tonk blues\n",
      "it's the honky tonk women\n",
      "gimme, gimme, gimme the honky tonk blues\n"
     ]
    }
   ],
   "source": [
    "testFile = open('../genres2/rock_lyrics/rock.00031.txt', 'r')\n",
    "testLyrics = testFile.read().lower()\n",
    "print(testLyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03946139 0.05489596 0.11682888 0.09060026 0.07516659 0.05440291\n",
      "  0.14750901 0.42113504]] rock\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "predictedLabels = []\n",
    "tk = Tokenizer()\n",
    "from statistics import mode\n",
    "\n",
    "labels = ['blues', 'country', 'disco', 'hiphop', 'metal','pop', 'reggae', 'rock']\n",
    "\n",
    "tk.fit_on_texts(testLyrics)\n",
    "index_list = tk.texts_to_sequences([testLyrics])\n",
    "# print(index_list)\n",
    "padded = pad_sequences(index_list, maxlen=1)\n",
    "pred = model.predict(padded)\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using LSTM trained on processed lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i met a gin-soaked, bar-room queen in memphis\n",
      "she tried to take me upstairs for a ride\n",
      "she had to heave me right across shoulder\n",
      "'cause i just can't seem to drink you off my mind\n",
      "it's the honky tonk women\n",
      "gimme, gimme, gimme the honky tonk blues\n",
      "i laid a divorcée in new york city\n",
      "i had to put up some kind of a fight\n",
      "the lady then she covered me with roses\n",
      "she blew my nose and then she blew my mind\n",
      "it's the honky tonk women\n",
      "gimme, gimme, gimme the honky tonk blues\n",
      "it's the honky tonk women\n",
      "gimme, gimme, gimme the honky tonk blues\n",
      "it's the honky tonk women\n",
      "gimme, gimme, gimme the honky tonk blues\n"
     ]
    }
   ],
   "source": [
    "testFile = open('../genres2/rock_lyrics/rock.00031.txt', 'r')\n",
    "testLyrics = testFile.read().lower()\n",
    "print(testLyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07003242 0.11136351 0.07551017 0.07906299 0.06030654 0.07478279\n",
      "  0.079055   0.44988665]] rock\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "predictedLabels = []\n",
    "tk = Tokenizer()\n",
    "from statistics import mode\n",
    "\n",
    "labels = ['blues', 'country', 'disco', 'hiphop', 'metal','pop', 'reggae', 'rock']\n",
    "\n",
    "tk.fit_on_texts(testLyrics)\n",
    "index_list = tk.texts_to_sequences([testLyrics])\n",
    "# print(index_list)\n",
    "padded = pad_sequences(index_list, maxlen=1)\n",
    "pred = modelP.predict(padded)\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
